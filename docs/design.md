# ðŸ§© Design Document

## Use Cases
1) Upload a PDF â†’ parse â†’ chunk â†’ index â†’ summarize.  
2) View document summary.  
3) Ask questions â†’ retrieve top-k chunks â†’ answer with citations.  

## Architecture
```
[Browser UI] --HTTP--> [FastAPI API]
 FastAPI -> PDF Parser (pypdf/pdfminer)
 FastAPI -> Chunker (sentence-aware)
 FastAPI -> TF-IDF Vector Store (scikit-learn)
 FastAPI -> Summarizer (frequency-based) or OpenAI (if configured)
 Persist: /app/data/<doc_id>/{meta.json, chunks.json, index.pkl, summary.txt}
```
Rationale: offlineâ€‘first, deterministic fallback; optional LLM improves quality but is not required.

## Data Model
- DocumentMeta: `{id, filename, created_at, chunk_size, overlap, num_chunks}`  
- chunks.json: `[{"idx":int,"text":str}, ...]`  
- index.pkl: scikitâ€‘learn vectorizer + matrix  
- summary.txt: extractive summary

## Sequence: Upload + Summarize
1. User uploads PDF (`POST /api/upload`).  
2. Server extracts text (pypdf â†’ pdfminer fallback).  
3. Split into sentences â†’ overlapping chunks (by words).  
4. TFâ€‘IDF fit & persist (`index.pkl`).  
5. Extractive summary â†’ save `summary.txt`.  
6. Return `doc_id`. Client fetches `GET /api/docs/{doc_id}/summary`.

## Sequence: Q&A
1. Client sends question + `doc_id` (`POST /api/chat`).  
2. Load TFâ€‘IDF index â†’ topâ€‘k chunks.  
3. If OpenAI API key present â†’ LLM answer from context. Else â†’ extractive stitch of relevant sentences.  
4. Return `answer` + `sources` (chunk, score).

## Wireframe (Loâ€‘Fi)
```
+---------------------------------------------+
| Upload PDF  [Choose File] [Upload]          |
+---------------------------------------------+
| Summary                                     |
|  <auto-generated summary here>              |
+---------------------------------------------+
| Chat                                        |
| Q: [type question...] [Send]                |
| A: <answer text>                            |
| Sources: [1] snippet...  [2] snippet...     |
+---------------------------------------------+
```

## Design Choices & Tradeoffs
- TFâ€‘IDF: deterministic, zero external dependencies, great for lexical queries.
- Optional OpenAI: better fluency; gated by API key.
- Persistence on disk: simple grading/inspection; no DB needed.
- Limitation: scanned PDFs not supported (no OCR in this starter).
